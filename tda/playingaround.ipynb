{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/rachelroca/Documents/GitHub/DAAAAMNdjmm/data cleaning/Final Coding.csv\",keep_default_na=False)\n",
    "#The codes were in 5 different columns; lets aggregate them into a new column\n",
    "df[\"all codes\"] = df[[\"SO code\",\"BC code\",\"BtC code\",\"CCM code\",\"SJEDI code\"]].agg(';'.join,axis=1)\n",
    "#We separated them by ;, so convert to an array \n",
    "df[\"all codes\"] = df[\"all codes\"].apply(lambda s: s.split(';'))\n",
    "#Strip out leading/trailing spaces\n",
    "df[\"all codes\"] = df[\"all codes\"].apply(lambda s: [t.strip() for t in s if t])\n",
    "#Sort the arrays, so we can be confident the same things are not listed in two different ways\n",
    "df[\"all codes\"] = np.array(df[\"all codes\"].apply(sorted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweeters = df['user_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BC impact',\n",
       " 'CC impact',\n",
       " 'SJEDI impact',\n",
       " 'SO impact',\n",
       " 'activation',\n",
       " 'advocacy',\n",
       " 'allyship in math',\n",
       " 'amplification',\n",
       " 'belonging',\n",
       " 'beyond',\n",
       " 'bipoc',\n",
       " 'celebration',\n",
       " 'conversation',\n",
       " 'current events',\n",
       " 'disrupteverything',\n",
       " 'economic',\n",
       " 'emotional support',\n",
       " 'futures',\n",
       " 'gender',\n",
       " 'hashtags',\n",
       " 'humanize',\n",
       " 'indigenous',\n",
       " 'influencers',\n",
       " 'jmm experience',\n",
       " 'learning',\n",
       " 'lgbtq',\n",
       " 'math experience',\n",
       " 'math practice',\n",
       " 'power',\n",
       " 'representation',\n",
       " 'strategic planning',\n",
       " 'talk support',\n",
       " 'thankful',\n",
       " 'troll',\n",
       " 'virtual participation']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes = ['advocacy', 'amplification', 'emotional support', 'gender', 'indigenous', 'jmm experience', 'representation', 'bipoc', 'hashtags', 'influencers', 'power', 'talk support', 'humanize', 'math practice', 'current events', 'lgbtq', 'strategic planning', 'allyship in math', 'math experience', 'conversation', 'thankful', 'BC impact', 'disrupteverything', 'SJEDI impact', 'SO impact', 'belonging', 'learning', 'troll', 'CC impact', 'virtual participation', 'activation', 'futures', 'economic', 'beyond', 'celebration']\n",
    "codes.sort()\n",
    "codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BC impact</th>\n",
       "      <th>CC impact</th>\n",
       "      <th>SJEDI impact</th>\n",
       "      <th>SO impact</th>\n",
       "      <th>activation</th>\n",
       "      <th>advocacy</th>\n",
       "      <th>allyship in math</th>\n",
       "      <th>amplification</th>\n",
       "      <th>belonging</th>\n",
       "      <th>beyond</th>\n",
       "      <th>...</th>\n",
       "      <th>lgbtq</th>\n",
       "      <th>math experience</th>\n",
       "      <th>math practice</th>\n",
       "      <th>power</th>\n",
       "      <th>representation</th>\n",
       "      <th>strategic planning</th>\n",
       "      <th>talk support</th>\n",
       "      <th>thankful</th>\n",
       "      <th>troll</th>\n",
       "      <th>virtual participation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dr. Adriana Salerno Dom√≠nguez üáªüá™</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Diaz Eaton - up to #GoodTrouble</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DR. ABOLISH THE POLICE</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prof. Christelle Vincent</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Wandering Point</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doctor.  Also a Bear.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Dougherty</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matt üè≥Ô∏è‚Äçüåà</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fumiko Futamura</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Noelle G. Beckman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows √ó 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    BC impact CC impact SJEDI impact  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™            1         0            0   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble        15         2            0   \n",
       "DR. ABOLISH THE POLICE                     15         2            1   \n",
       "Prof. Christelle Vincent                    1         0            0   \n",
       "Dr. Wandering Point                         3         1            0   \n",
       "...                                       ...       ...          ...   \n",
       "Doctor.  Also a Bear.                       0         0            0   \n",
       "Daniel Dougherty                            0         0            0   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                   0         0            0   \n",
       "Fumiko Futamura                             0         0            0   \n",
       "Dr. Noelle G. Beckman                       0         0            0   \n",
       "\n",
       "                                    SO impact activation advocacy  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™            0          0        1   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble         3          1        7   \n",
       "DR. ABOLISH THE POLICE                      6          1        8   \n",
       "Prof. Christelle Vincent                    1          0        2   \n",
       "Dr. Wandering Point                         2          0        1   \n",
       "...                                       ...        ...      ...   \n",
       "Doctor.  Also a Bear.                       0          0        1   \n",
       "Daniel Dougherty                            0          0        1   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                   0          0        1   \n",
       "Fumiko Futamura                             0          0        1   \n",
       "Dr. Noelle G. Beckman                       0          0        0   \n",
       "\n",
       "                                    allyship in math amplification belonging  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™                   0             6         0   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble                5            69         4   \n",
       "DR. ABOLISH THE POLICE                             1            51         5   \n",
       "Prof. Christelle Vincent                           1             6         0   \n",
       "Dr. Wandering Point                                1            11         0   \n",
       "...                                              ...           ...       ...   \n",
       "Doctor.  Also a Bear.                              0             1         0   \n",
       "Daniel Dougherty                                   0             1         0   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                          0             1         0   \n",
       "Fumiko Futamura                                    0             1         0   \n",
       "Dr. Noelle G. Beckman                              0             0         0   \n",
       "\n",
       "                                    beyond  ... lgbtq math experience  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™         0  ...     1               0   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble      0  ...     4               2   \n",
       "DR. ABOLISH THE POLICE                   0  ...     7               2   \n",
       "Prof. Christelle Vincent                 0  ...     0               0   \n",
       "Dr. Wandering Point                      0  ...     0               0   \n",
       "...                                    ...  ...   ...             ...   \n",
       "Doctor.  Also a Bear.                    0  ...     0               0   \n",
       "Daniel Dougherty                         0  ...     0               0   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                0  ...     0               0   \n",
       "Fumiko Futamura                          0  ...     0               0   \n",
       "Dr. Noelle G. Beckman                    0  ...     0               0   \n",
       "\n",
       "                                    math practice power representation  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™                2     1              5   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble             9    16             25   \n",
       "DR. ABOLISH THE POLICE                         11    15             20   \n",
       "Prof. Christelle Vincent                        0     2              6   \n",
       "Dr. Wandering Point                             1     3              6   \n",
       "...                                           ...   ...            ...   \n",
       "Doctor.  Also a Bear.                           0     0              1   \n",
       "Daniel Dougherty                                0     0              1   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                       0     0              1   \n",
       "Fumiko Futamura                                 0     0              1   \n",
       "Dr. Noelle G. Beckman                           0     0              0   \n",
       "\n",
       "                                    strategic planning talk support thankful  \\\n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™                     2            6        0   \n",
       "Dr. Diaz Eaton - up to #GoodTrouble                 17           59       12   \n",
       "DR. ABOLISH THE POLICE                              15           42        9   \n",
       "Prof. Christelle Vincent                             3            4        1   \n",
       "Dr. Wandering Point                                  1           10        0   \n",
       "...                                                ...          ...      ...   \n",
       "Doctor.  Also a Bear.                                0            0        0   \n",
       "Daniel Dougherty                                     0            0        0   \n",
       "Matt üè≥Ô∏è‚Äçüåà                                            0            0        0   \n",
       "Fumiko Futamura                                      0            0        0   \n",
       "Dr. Noelle G. Beckman                                0            1        0   \n",
       "\n",
       "                                    troll virtual participation  \n",
       "Dr. Adriana Salerno Dom√≠nguez üáªüá™        0                     0  \n",
       "Dr. Diaz Eaton - up to #GoodTrouble     0                     1  \n",
       "DR. ABOLISH THE POLICE                  0                     0  \n",
       "Prof. Christelle Vincent                0                     0  \n",
       "Dr. Wandering Point                     0                     0  \n",
       "...                                   ...                   ...  \n",
       "Doctor.  Also a Bear.                   0                     0  \n",
       "Daniel Dougherty                        0                     0  \n",
       "Matt üè≥Ô∏è‚Äçüåà                               0                     0  \n",
       "Fumiko Futamura                         0                     0  \n",
       "Dr. Noelle G. Beckman                   0                     0  \n",
       "\n",
       "[327 rows x 35 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_codes = pd.DataFrame(columns = codes)\n",
    "for i in range(len(tweeters)):\n",
    "    df_temp = df[df['user_name'] == tweeters[i]]\n",
    "    #sum the all codes column\n",
    "    codes_sum_temp = df_temp['all codes'].sum()\n",
    "    #make a dictionary of the codes and how many times they appear in codes_sum_temp\n",
    "    codes_dict_temp = {i:codes_sum_temp.count(i) for i in codes}\n",
    "    #make the dictionary a dataframe\n",
    "    codes_dict_temp_df = pd.DataFrame.from_dict([codes_dict_temp])\n",
    "    #add the dictionary to the dataframe with the tweeter's name as index\n",
    "    df_codes = pd.concat([df_codes,codes_dict_temp_df], ignore_index=True)\n",
    "#make the indices the tweeters' names\n",
    "df_codes.index = tweeters\n",
    "df_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [15, 2, 0, ..., 12, 0, 1],\n",
       "       [15, 2, 1, ..., 9, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make the dataframe into a numpy array\n",
    "df_codes_array = df_codes.to_numpy()\n",
    "df_codes_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
